---
layout: technical
title: Reinforcement Learning With Human Feedback
category: Training and Learning
difficulty: Advanced
description: Discussions around Layer Norm
show_back_link: true
---

## Reinforcement Learning With Human Feedback
RLHF Step 1: Supervised finetuning of the pretrained model
RLHF Step 2: Creating a reward model
RLHF Step 3: Finetuning via proximal policy optimization

### Proximal Policy Optimization